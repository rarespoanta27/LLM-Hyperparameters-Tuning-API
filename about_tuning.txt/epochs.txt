Epochs reprezintă un parametru esențial în antrenarea unui model de învățare automată, referindu-se la numărul de treceri complete pe care modelul le face prin întregul set de date de antrenament.

Ce este o epochă?
Epoch se referă la o trecere completă a modelului prin toate exemplele din setul de date de antrenament.
În cadrul unei epoci, modelul trece prin fiecare eșantion din setul de date și ajustează parametrii în funcție de rezultatul funcției de pierdere.
O epochă este diferită de un batch. Dacă setul de date este mare, acesta poate fi împărțit în batch-uri mai mici, iar o epochă implică procesarea tuturor batch-urilor din setul de date o singură dată.
Cum funcționează o epochă?
Setul de date de antrenament este împărțit în mai multe batch-uri mai mici.
Pentru fiecare batch, modelul face o predicție, iar funcția de pierdere măsoară eroarea dintre predicțiile modelului și etichetele reale.
Optimizatorul folosește această eroare pentru a ajusta greutățile și parametrii modelului, astfel încât să îmbunătățească predicțiile pentru batch-urile viitoare.
După ce toate batch-urile din setul de date au fost procesate, se încheie o epochă.
După fiecare epochă, modelul poate fi evaluat pe setul de validare sau pe un set de testare pentru a măsura progresul.
De ce este importantă numărul de epoci?
Numărul de epoci influențează performanța și acuratețea modelului. Dacă alegi un număr prea mic de epoci, modelul poate să nu aibă suficient timp pentru a învăța relațiile dintre variabile și ar putea subantrena (underfitting). Dacă alegi un număr prea mare de epoci, modelul poate învăța prea bine datele de antrenament și să suprantreneze (overfitting), ceea ce înseamnă că va performa bine pe setul de date de antrenament, dar prost pe date noi.

Evoluția modelului pe parcursul epocilor
În mod obișnuit, pe măsură ce numărul de epoci crește, modelul devine mai bun în ajustarea parametrilor și îmbunătățirea performanței.
Funcția de pierdere tinde să scadă pe măsură ce modelul parcurge mai multe epoci și învață din greșeli.
Cu toate acestea, după un anumit număr de epoci, modelul poate începe să suprantreneze, învățând prea specific detaliile din setul de antrenament.
Alegerea numărului potrivit de epoci
Numărul optim de epoci depinde de mai mulți factori, inclusiv complexitatea datelor, dimensiunea setului de date și complexitatea modelului.

Cross-validation și early stopping sunt tehnici utilizate frecvent pentru a determina când să se oprească antrenarea.
Early stopping este o tehnică prin care antrenarea este oprită atunci când performanța modelului pe setul de validare nu se mai îmbunătățește, evitând astfel suprantrenarea.
Exemplu
Dacă ai un set de date cu 1000 de exemple și utilizezi un batch size de 100, atunci o epochă va fi completă după 10 pași (1000/100).
Dacă setezi 50 de epoci, modelul va trece prin întregul set de date de 50 de ori, ajustându-și parametrii de fiecare dată.
Impactul numărului de epoci asupra performanței
Prea puține epoci: Modelul nu are suficient timp să învețe. Va avea o performanță slabă atât pe setul de antrenament, cât și pe setul de testare (underfitting).
Prea multe epoci: Modelul învață prea bine datele de antrenament, ceea ce duce la performanțe excelente pe setul de antrenament, dar slabe pe date noi (overfitting).
Numărul optim de epoci: Acesta se găsește prin experimentare. În mod ideal, modelul ar trebui să învețe suficient din date fără a deveni prea specializat pe setul de antrenament.
În concluzie:
Epoch-urile reprezintă numărul de treceri pe care modelul le face prin întregul set de date de antrenament. Alegerea unui număr adecvat de epoci este esențială pentru a evita subantrenarea sau suprantrenarea modelului. Tehnici precum early stopping pot ajuta la determinarea momentului optim pentru a opri antrenarea modelului.